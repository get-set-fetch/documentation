<h3 class="title" id="site">Site Entity</h3>
<hr/>

<p>
Each crawl process starts by defining a site with a base url from where resources will be identified and then processed.<br/>
The site url is used to create the first site resource with depth 0.<br/><br/>

Each site has its own set of plugins and plugin options, making the crawl process customizable at site level.<br/>
Plugins and corresponding options are persisted, so after you've defined a site, you only have to retrieve it to continue crawling.<br/><br/>

During SELECT phase a resource is selected for crawling.<br/>
During SAVE phase the current resource is updated and the newly found resources are added.<br/>
Pairwise independent bloom filters are used to determine if a given resource has already been added.<br/><br/>

You can customize the storage behavior by writing your own plugins for the SELECT and SAVE phases. <br/>
</p>

<h5 class="subtitle">Persisted properties</h5>
<table class="table">
    <thead>
        <tr>
        <th scope="col">Name</th>
        <th scope="col">Type</th>
        <th scope="col">Description</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <th scope="row">id</th>
            <td>number | objectId</td>
            <td>unique identifier, number for sql engines, objectId for mongodb</td>            
        </tr>
        <tr>
            <th scope="row">name</th>
            <td>string</td>
            <td>unique name for the site to crawl, a site can be retrieved by name or id</td>            
        </tr>
        <tr>
            <th scope="row">url</th>
            <td>string</td>
            <td>
                crawling will start from this URL "downwards".<br/>
                Example: site url is http://siteA/path1/.<br/>
                Resources under http://siteA/path1/ or http://siteA/path1/subpath2/ will be crawled.<br/>
                Higher resources like http://siteA/path2 will not be crawled.<br/>
                If you want to crawl an entire site and not just a section of it, have the url point to the site domain name.
            </td>            
        </tr>
        <tr>
            <th scope="row">robotsTxt</th>
            <td>string</td>
            <td>
                site robots.txt content
            </td>            
        </tr>
        <tr>
            <th scope="row">resourceFilter</th>
            <td>buffer</td>
            <td>
                bloom filter for detecting duplicate URLs when adding new resources to crawl
            </td>            
        </tr>
        <tr>
            <th scope="row">plugins</th>
            <td>json</td>
            <td>
                array of {name, opts} representing the site plugins.<br/>
                name represents the plugin constructor name. <br/>
                opts represents the plugin options. <br/>
                Site.crawlResource will execute the plugins against the currently crawled resource.
            </td>            
        </tr>
    </tbody>
</table>

<h5 class="subtitle">Transient properties</h5>
<table class="table">
    <thead>
        <tr>
        <th scope="col">Name</th>
        <th scope="col">Type</th>
        <th scope="col">Description</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td colspan="3"><i>No transient properties.</i></td>            
        </tr>
    </tbody>
</table>