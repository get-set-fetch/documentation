---
title: Getting Started
---
<p>
  You start by creating a project with name, description, base url, crawl options and scraping scenario.
</p>
<p>
  After the project is created you can start scraping from the selected base url.
</p>
<p>
  Once scraping is complete you can view the results and export them. All scenarios support scraping javascript based, dynamically generated pages.
</p>

<p>
  Available crawl options:
</p>
<table class="table">
  <thead>
    <tr>
      <th scope="col">Crawl option</th>
      <th scope="col">Description</th>
      <th scope="col">Default value</th>
    </tr>
  </thead>
  <tbody>
  <tr>
    <td>Maximum crawl depth</td>
    <td>
      The extent to which new resources are discovered.<br/>
      Number of clicks needed to reach a resource starting from the project start url.<br/>
      Start URL has depth of 0. All resources linked from it have depth of 1 and so on. <br/>
      A value of -1 denotes unlimited crawl depth.
    </td>
    <td>
      <ul>
        <li>
          -1<br/>
          unlimited crawl depth
        </li>
      </ul>
    </td>
  </tr>
  <tr>
    <td>Maximum resources</td>
    <td>
      Maximum number of resources to be crawled.
    </td>
    <td>
      <ul>
        <li>
          100
        </li>
      </ul>
    </td>
  </tr>
  <tr>
    <td>Crawl delay</td>
    <td>
      Delay in milliseconds between crawling two consecutive resources.
    </td>
    <td>
      <ul>
        <li>
          1000
        </li>
      </ul>
    </td>
  </tr>
  <tr>
    <td>Domain regexp filter</td>
    <td>
      Based on URL hostname, filter which html pages should be queued for crawling.<br/>
      Leave empty to only add resources from the same domain (including subdomains).<br/>
      Ex: /\.org$ only links from .org domains will be crawled.
    </td>
    <td>
      <ul>
        <li>
          &lt;empty&gt;
        </li>
      </ul>
    </td>
  </tr>
  <tr>
    <td>Pathname regexp filter</td>
    <td>
      Based on URL pathname, filter which html pages should be queued for crawling.<br/>
      Leave empty to add all links.<br/>
      Ex: /product/ - only links like extra-product-extra.html will be crawled.<br/>
      Ex: /category\/product/ - only links like extra-category/product-extra.html will be crawled.
    </td>
    <td>
      <ul>
        <li>
          &lt;empty&gt;
        </li>
      </ul>
    </td>
  </tr>
</table>

